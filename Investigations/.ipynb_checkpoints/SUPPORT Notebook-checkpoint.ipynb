{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5c44d2",
   "metadata": {},
   "source": [
    "# SUPPORT Notebook\n",
    "\n",
    "This notebook runs through investigations on the open access SUPPORT dataset.\n",
    "\n",
    "For users who do not have lots of computational resources or do not have access to MIMIC-III then this notebook should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Import Libraries -------- #\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# VAE is in other folder\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "# For the SUPPORT dataset\n",
    "from pycox.datasets import support\n",
    "\n",
    "# For VAE dataset formatting\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# VAE functions\n",
    "from VAE import Decoder, Encoder, VAE\n",
    "\n",
    "from utils import support_pre_proc, plot_elbo, plot_likelihood_breakdown, plot_variable_distributions, metric_calculation, reverse_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc97da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the support data\n",
    "data_supp = support.read_df()\n",
    "\n",
    "# Column Definitions\n",
    "original_continuous_columns = ['duration'] + [f\"x{i}\" for i in range(7,15)]\n",
    "original_categorical_columns = ['event'] + [f\"x{i}\" for i in range(1,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Data Pre-Processing -------- #\n",
    "\n",
    "pre_proc_method = \"GMM\"\n",
    "\n",
    "x_train, data_supp, reordered_dataframe_columns, continuous_transformers, categorical_transformers, num_categories, num_continuous = support_pre_proc(data_supp=data_supp, pre_proc_method=pre_proc_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbd19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Create & Train VAE -------- #\n",
    "\n",
    "# User defined hyperparams\n",
    "# General training\n",
    "batch_size=32\n",
    "latent_dim=256\n",
    "hidden_dim=256\n",
    "n_epochs=5\n",
    "logging_freq=1 # Number of epochs we should log the results to the user\n",
    "patience=5 # How many epochs should we allow the model train to see if\n",
    "# improvement is made\n",
    "delta=10 # The difference between elbo values that registers an improvement\n",
    "filepath=None # Where to save the best model\n",
    "\n",
    "\n",
    "# Privacy params\n",
    "differential_privacy = False # Do we want to implement differential privacy\n",
    "sample_rate=0.1 # Sampling rate\n",
    "C = 1e16 # Clipping threshold - any gradients above this are clipped\n",
    "noise_scale=None # Noise multiplier - influences how much noise to add\n",
    "target_eps=1 # Target epsilon for privacy accountant\n",
    "target_delta=1e-5 # Target delta for privacy accountant\n",
    "\n",
    "# Prepare data for interaction with torch VAE\n",
    "Y = torch.Tensor(x_train)\n",
    "dataset = TensorDataset(Y)\n",
    "\n",
    "generator = None\n",
    "sample_rate = batch_size / len(dataset)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_sampler=UniformWithReplacementSampler(\n",
    "        num_samples=len(dataset), sample_rate=sample_rate, generator=generator\n",
    "    ),\n",
    "    pin_memory=True,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "# Create VAE\n",
    "encoder = Encoder(x_train.shape[1], latent_dim, hidden_dim=hidden_dim)\n",
    "decoder = Decoder(\n",
    "    latent_dim, num_continuous, num_categories=num_categories\n",
    ")\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "if(differential_privacy==False):\n",
    "    log_elbo, log_reconstruction, log_divergence, log_categorical, log_numerical = vae.train(data_loader, n_epochs=n_epochs)\n",
    "    \n",
    "elif(differential_privacy==True):\n",
    "    log_elbo, log_reconstruction, log_divergence, log_categorical, log_numerical = vae.diff_priv_train(\n",
    "            data_loader,\n",
    "            n_epochs=n_epochs,\n",
    "            C=C,\n",
    "            target_eps=target_eps,\n",
    "            target_delta=target_delta,\n",
    "            sample_rate=sample_rate,\n",
    "            noise_scale=noise_scale\n",
    "        )\n",
    "    print(f\"(epsilon, delta): {vae.get_privacy_spent(target_delta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ee7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Loss Features ELBO Breakdown -------- #\n",
    "\n",
    "plot_elbo(\n",
    "    n_epochs=n_epochs, log_elbo=log_elbo, log_reconstruction=log_reconstruction,\n",
    "    log_divergence=log_divergence, saving_filepath=\"\", pre_proc_method=pre_proc_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Loss Features Reconstruction Breakdown -------- #\n",
    "\n",
    "plot_likelihood_breakdown(\n",
    "    n_epochs=n_epochs, log_categorical=log_categorical, log_numerical=log_numerical,\n",
    "    saving_filepath=\"\", pre_proc_method=pre_proc_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Synthetic Data Generation -------- #\n",
    "\n",
    "synthetic_sample = vae.generate(data_supp.shape[0])\n",
    "\n",
    "synthetic_sample = pd.DataFrame(synthetic_sample.cpu().detach(), columns=reordered_dataframe_columns)\n",
    "\n",
    "# Reverse the transformations\n",
    "\n",
    "synthetic_supp = reverse_transformers(synthetic_set=synthetic_sample, data_supp_columns=data_supp.columns, \n",
    "                                      cont_transformers=continuous_transformers, cat_transformers=categorical_transformers,\n",
    "                                      pre_proc_method=pre_proc_method\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93056a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- Plot Histograms For All The Variable Distributions -------- #\n",
    "\n",
    "plot_variable_distributions(\n",
    "    categorical_columns=original_categorical_columns, continuous_columns=original_continuous_columns,\n",
    "    data_supp=data_supp, synthetic_supp=synthetic_supp,saving_filepath=\"\",\n",
    "    pre_proc_method=pre_proc_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09966e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- SDV Metrics -------- #\n",
    "\n",
    "# Define the metrics you want the model to evaluate\n",
    "\n",
    "user_metrics = ['ContinuousKLDivergence', 'DiscreteKLDivergence']\n",
    "\n",
    "metrics = metric_calculation(\n",
    "    user_metrics=user_metrics, data_supp=data_supp, synthetic_supp=synthetic_supp,\n",
    "    categorical_columns=original_categorical_columns, continuous_columns=original_continuous_columns,\n",
    "    saving_filepath=\"\", pre_proc_method=pre_proc_method\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
